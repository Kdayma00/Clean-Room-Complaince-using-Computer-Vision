# -*- coding: utf-8 -*-
"""clean room.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mwwpgHqTd-j-n0Xm75mCHt8hXaekx-GF
"""

pip install opencv-python mediapipe ultralytics

import cv2
import mediapipe as mp
import time
from ultralytics import YOLO

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# Initialize YOLO Model
yolo_model = YOLO("trainedyolo11n.pt")
desired_classes = [1, 2, 4, 5]  # Define classes to detect: 1 - Hairnet, 2 - Mask, etc.

from google.colab import files

uploaded = files.upload()

for filename in uploaded.keys():
    print(f'User uploaded file "{filename}" with length {len(uploaded[filename])} bytes')
    video_filename = filename # Store the name of the uploaded video file

cap = cv2.VideoCapture(video_filename)

def is_hand_in_zone(hand_landmarks, frame_width, frame_height, zone_x_min, zone_y_min, zone_x_max, zone_y_max):
    for landmark in hand_landmarks.landmark:
        if (zone_x_min <= landmark.x * frame_width <= zone_x_max and zone_y_min <= landmark.y * frame_height <= zone_y_max):
            return True
    return False

from IPython.display import display, Image
import io

# Initialize variables
hand_detected_time = None
hands_detected_in_zone = False
frame_counter = 0  # Initialize a frame counter

# Start MediaPipe hands
with mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.1, min_tracking_confidence=0.1) as hands:
    # Initialize video capture inside the processing cell
    cap = cv2.VideoCapture(video_filename)

    if not cap.isOpened():
        print(f"Error: Could not open video file '{video_filename}'. Please check if the file exists and is a valid video format.")
        # Optionally, you might want to exit or raise an error here
        # For now, we'll just print and let the loop proceed, which will likely break immediately.

    try:
        while True:
            # To analyze only every 3rd frame
            if frame_counter % 3 == 0:
                ret, frame = cap.read()
                if not ret:
                    print("Failed to grab frame or end of video stream.")
                    break

                frame_height, frame_width, _ = frame.shape

                # Define the zone
                zone_x_min = int(frame_width * 0.1)
                zone_y_min = int(frame_height * 0.65)
                zone_x_max = int(frame_width * 0.9)
                zone_y_max = int(frame_height * 1)
                cv2.rectangle(frame, (zone_x_min, zone_y_min), (zone_x_max, zone_y_max), (255, 0, 0), 2)

                # Convert the BGR frame to RGB for MediaPipe
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # Process the frame and detect hands
                results = hands.process(rgb_frame)

                hand_in_zone = False
                if results.multi_hand_landmarks:
                    for hand_landmarks in results.multi_hand_landmarks:
                        if is_hand_in_zone(hand_landmarks, frame_width, frame_height, zone_x_min, zone_y_min, zone_x_max, zone_y_max):
                            hand_in_zone = True
                            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

                hands_detected_in_zone = hand_in_zone

                # Check time duration for hand detection in the zone
                if hands_detected_in_zone:
                    if hand_detected_time is None:
                        hand_detected_time = time.time()
                    elapsed_time = time.time() - hand_detected_time
                    countdown = max(0, 5 - int(elapsed_time))
                    if countdown == 0:
                        message = "Thanks for washing for 5 seconds!"
                    else:
                        message = f"Counting down: {countdown}s"
                else:
                    hand_detected_time = None
                    countdown = 5
                    message = "No hands detected in the zone."

                # Display the message and countdown on the frame
                cv2.putText(frame, message, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv2.LINE_AA)

                # YOLO Object Detection
                yolo_results = yolo_model(frame)

                # Filter results to only include desired classes
                for result in yolo_results:
                    for box in result.boxes:
                        if box.cls.item() in desired_classes:
                            x1, y1, x2, y2 = [int(coord) for coord in box.xyxy[0]]
                            label = yolo_model.names[int(box.cls)]
                            if box.cls.item() == 1 or box.cls.item() == 2:
                                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                                cv2.putText(frame, f"{label}", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX , 0.5,(0, 255, 0), 2)
                            else:
                                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
                                cv2.putText(frame, f"{label}", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX , 0.5, (0, 0, 255), 2)

                # Convert the frame to JPEG and display in Colab
                _, buffer = cv2.imencode('.jpg', frame)
                io_buf = io.BytesIO(buffer)
                display(Image(io_buf.getvalue()))

                # Increment the frame counter
                frame_counter += 1

            # You might need a small delay here for display to catch up
            time.sleep(0.01)

    except KeyboardInterrupt:
        print("Video processing stopped by user.")
    finally:
        # Release the video capture object and close any remaining windows
        cap.release()
        cv2.destroyAllWindows()

"""After successfully uploading a new video file, please run cell `j3XZTsRX-nSK` again to start the processing with your new video.

After uploading the video file, you'll need to modify the `cap = cv2.VideoCapture(0)` line in cell `DPJHu422-n5J` to `cap = cv2.VideoCapture(video_filename)` (replace `video_filename` with the actual name of your uploaded video file). Then, you can run cell `j3XZTsRX-nSK` again.
"""